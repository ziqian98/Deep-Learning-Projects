{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation\n",
    "import torch.optim as optim\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(cuda)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy',allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz',allow_pickle=True)  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy',allow_pickle=True)  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz',allow_pickle=True)  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy',allow_pickle=True)  # test\n",
    "vocab = np.load('../dataset/vocab.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        #raise NotImplemented\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sequenceLen = 200\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        #raise NotImplemented\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.dataset)\n",
    "        \n",
    "        dataArray = np.concatenate(self.dataset)\n",
    "        dataMatrix = torch.LongTensor(dataArray[0:(dataArray.shape[0] // self.sequenceLen) * self.sequenceLen].reshape(dataArray.shape[0] // self.sequenceLen,self.sequenceLen))\n",
    "        \n",
    "        i = 0\n",
    "        while i<dataMatrix.shape[0]:\n",
    "            batchMatrix = dataMatrix[i:i+self.batch_size]\n",
    "            i = i + self.batch_size\n",
    "            inputx = batchMatrix[:,:batchMatrix.size()[1]-1]\n",
    "            inputy = batchMatrix[:,1:]\n",
    "            \n",
    "            yield (inputx,inputy)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        #raise NotImplemented\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 256)\n",
    "        self.lstm = nn.LSTM(input_size = 256, hidden_size = 256, num_layers = 3, dropout = 0.2, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(256, vocab_size)\n",
    "        self.embedding.weight = self.linear.weight\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-0.1,0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1,0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        #raise NotImplemented\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x,y = self.lstm(x)\n",
    "        \n",
    "        x = x.contiguous()\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model.to(device)\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(),lr=5.0,weight_decay=1e-6)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,mode=\"min\",factor=0.8,patience=1)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            if batch_num % 50 == 0:\n",
    "                print(\"batch_num: \" , batch_num)\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = self.model(inputs)\n",
    "        \n",
    "        loss = self.criterion(outputs.view(-1, outputs.size(2)), targets.contiguous().view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        epo_loss = loss.item()\n",
    "        \n",
    "        return epo_loss\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        self.scheduler.step(nll)\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        inp = torch.LongTensor(inp).to(device)\n",
    "        model.to(device)\n",
    "        outputs = model(inp)\n",
    "        \n",
    "        return outputs[:, -1, :].detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        #raise NotImplemented\n",
    "        generated_words = []\n",
    "        inp = torch.LongTensor(inp).to(device)\n",
    "        model.to(device)\n",
    "        embedded = model.embedding(inp)\n",
    "        output_lstm, hidden = model.lstm(embedded)\n",
    "        scores = model.linear(output_lstm[:, -1, :])\n",
    "        _, current_word = torch.max(scores, dim=1)\n",
    "        generated_words.append(current_word)\n",
    "        \n",
    "        for i in range(forward-1):\n",
    "            embedded = model.embedding(current_word).unsqueeze(1)\n",
    "            output_lstm, hidden = model.lstm(embedded, hidden)\n",
    "            scores = model.linear(output_lstm[:, -1, :])\n",
    "            _, current_word = torch.max(scores, dim=1)\n",
    "            generated_words.append(current_word)\n",
    "        \n",
    "        predicts = torch.stack(generated_words).permute(1, 0)\n",
    "        \n",
    "        return predicts.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = 999\n",
    "BATCH_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1588481319\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [2/999]   Loss: 7.8459\n",
      "[VAL]  Epoch [2/999]   Loss: 7.3095\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 7.3095293\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [3/999]   Loss: 7.7354\n",
      "[VAL]  Epoch [3/999]   Loss: 7.0775\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 7.077526\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [4/999]   Loss: 7.9196\n",
      "[VAL]  Epoch [4/999]   Loss: 7.1486\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [5/999]   Loss: 7.4645\n",
      "[VAL]  Epoch [5/999]   Loss: 6.7180\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 6.718029\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [6/999]   Loss: 7.2644\n",
      "[VAL]  Epoch [6/999]   Loss: 6.5729\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 6.572938\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [7/999]   Loss: 7.1620\n",
      "[VAL]  Epoch [7/999]   Loss: 6.5199\n",
      "Saving model, predictions and generated output for epoch 5 with NLL: 6.5199385\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [8/999]   Loss: 7.0669\n",
      "[VAL]  Epoch [8/999]   Loss: 6.3826\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 6.3826456\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [9/999]   Loss: 6.9400\n",
      "[VAL]  Epoch [9/999]   Loss: 6.3281\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 6.3281054\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [10/999]   Loss: 6.8273\n",
      "[VAL]  Epoch [10/999]   Loss: 6.1701\n",
      "Saving model, predictions and generated output for epoch 8 with NLL: 6.1700697\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [11/999]   Loss: 6.7154\n",
      "[VAL]  Epoch [11/999]   Loss: 6.1076\n",
      "Saving model, predictions and generated output for epoch 9 with NLL: 6.107596\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [12/999]   Loss: 6.6160\n",
      "[VAL]  Epoch [12/999]   Loss: 6.0463\n",
      "Saving model, predictions and generated output for epoch 10 with NLL: 6.046303\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [13/999]   Loss: 6.5236\n",
      "[VAL]  Epoch [13/999]   Loss: 6.0171\n",
      "Saving model, predictions and generated output for epoch 11 with NLL: 6.0170565\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [14/999]   Loss: 6.4426\n",
      "[VAL]  Epoch [14/999]   Loss: 5.9083\n",
      "Saving model, predictions and generated output for epoch 12 with NLL: 5.908293\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [15/999]   Loss: 6.3701\n",
      "[VAL]  Epoch [15/999]   Loss: 5.8246\n",
      "Saving model, predictions and generated output for epoch 13 with NLL: 5.824627\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [16/999]   Loss: 6.3014\n",
      "[VAL]  Epoch [16/999]   Loss: 5.7328\n",
      "Saving model, predictions and generated output for epoch 14 with NLL: 5.732788\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [17/999]   Loss: 6.2420\n",
      "[VAL]  Epoch [17/999]   Loss: 5.7229\n",
      "Saving model, predictions and generated output for epoch 15 with NLL: 5.7228765\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [18/999]   Loss: 6.1729\n",
      "[VAL]  Epoch [18/999]   Loss: 5.7091\n",
      "Saving model, predictions and generated output for epoch 16 with NLL: 5.7091475\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [19/999]   Loss: 6.1132\n",
      "[VAL]  Epoch [19/999]   Loss: 5.5818\n",
      "Saving model, predictions and generated output for epoch 17 with NLL: 5.581791\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [20/999]   Loss: 6.0633\n",
      "[VAL]  Epoch [20/999]   Loss: 5.5567\n",
      "Saving model, predictions and generated output for epoch 18 with NLL: 5.5566654\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [21/999]   Loss: 6.0159\n",
      "[VAL]  Epoch [21/999]   Loss: 5.4693\n",
      "Saving model, predictions and generated output for epoch 19 with NLL: 5.4693103\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [22/999]   Loss: 5.9641\n",
      "[VAL]  Epoch [22/999]   Loss: 5.5465\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [23/999]   Loss: 5.9229\n",
      "[VAL]  Epoch [23/999]   Loss: 5.4018\n",
      "Saving model, predictions and generated output for epoch 21 with NLL: 5.4017963\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [24/999]   Loss: 5.8813\n",
      "[VAL]  Epoch [24/999]   Loss: 5.4437\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [25/999]   Loss: 5.8443\n",
      "[VAL]  Epoch [25/999]   Loss: 5.4662\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [26/999]   Loss: 5.7669\n",
      "[VAL]  Epoch [26/999]   Loss: 5.3585\n",
      "Saving model, predictions and generated output for epoch 24 with NLL: 5.358525\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [27/999]   Loss: 5.7237\n",
      "[VAL]  Epoch [27/999]   Loss: 5.3351\n",
      "Saving model, predictions and generated output for epoch 25 with NLL: 5.3351307\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [28/999]   Loss: 5.6911\n",
      "[VAL]  Epoch [28/999]   Loss: 5.2716\n",
      "Saving model, predictions and generated output for epoch 26 with NLL: 5.2715807\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [29/999]   Loss: 5.6599\n",
      "[VAL]  Epoch [29/999]   Loss: 5.3221\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [30/999]   Loss: 5.6221\n",
      "[VAL]  Epoch [30/999]   Loss: 5.2724\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [31/999]   Loss: 5.5571\n",
      "[VAL]  Epoch [31/999]   Loss: 5.1942\n",
      "Saving model, predictions and generated output for epoch 29 with NLL: 5.19417\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [32/999]   Loss: 5.5199\n",
      "[VAL]  Epoch [32/999]   Loss: 5.2461\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [33/999]   Loss: 5.4845\n",
      "[VAL]  Epoch [33/999]   Loss: 5.1851\n",
      "Saving model, predictions and generated output for epoch 31 with NLL: 5.185053\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [34/999]   Loss: 5.4571\n",
      "[VAL]  Epoch [34/999]   Loss: 5.1621\n",
      "Saving model, predictions and generated output for epoch 32 with NLL: 5.162101\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [35/999]   Loss: 5.4225\n",
      "[VAL]  Epoch [35/999]   Loss: 5.1540\n",
      "Saving model, predictions and generated output for epoch 33 with NLL: 5.15405\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [36/999]   Loss: 5.3928\n",
      "[VAL]  Epoch [36/999]   Loss: 5.1502\n",
      "Saving model, predictions and generated output for epoch 34 with NLL: 5.1501985\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [37/999]   Loss: 5.3679\n",
      "[VAL]  Epoch [37/999]   Loss: 5.1069\n",
      "Saving model, predictions and generated output for epoch 35 with NLL: 5.1068716\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [38/999]   Loss: 5.3419\n",
      "[VAL]  Epoch [38/999]   Loss: 5.0770\n",
      "Saving model, predictions and generated output for epoch 36 with NLL: 5.076989\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [39/999]   Loss: 5.3147\n",
      "[VAL]  Epoch [39/999]   Loss: 5.0768\n",
      "Saving model, predictions and generated output for epoch 37 with NLL: 5.076783\n",
      "batch_num:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [40/999]   Loss: 5.2877\n",
      "[VAL]  Epoch [40/999]   Loss: 5.0334\n",
      "Saving model, predictions and generated output for epoch 38 with NLL: 5.0333805\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [41/999]   Loss: 5.2662\n",
      "[VAL]  Epoch [41/999]   Loss: 5.0248\n",
      "Saving model, predictions and generated output for epoch 39 with NLL: 5.0247993\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [42/999]   Loss: 5.2405\n",
      "[VAL]  Epoch [42/999]   Loss: 4.9610\n",
      "Saving model, predictions and generated output for epoch 40 with NLL: 4.96103\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [43/999]   Loss: 5.2181\n",
      "[VAL]  Epoch [43/999]   Loss: 5.0116\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [44/999]   Loss: 5.1993\n",
      "[VAL]  Epoch [44/999]   Loss: 4.9767\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [45/999]   Loss: 5.1420\n",
      "[VAL]  Epoch [45/999]   Loss: 4.9849\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [46/999]   Loss: 5.1135\n",
      "[VAL]  Epoch [46/999]   Loss: 4.9220\n",
      "Saving model, predictions and generated output for epoch 44 with NLL: 4.9219584\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [47/999]   Loss: 5.1039\n",
      "[VAL]  Epoch [47/999]   Loss: 4.8967\n",
      "Saving model, predictions and generated output for epoch 45 with NLL: 4.8967247\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [48/999]   Loss: 5.0763\n",
      "[VAL]  Epoch [48/999]   Loss: 4.9546\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [49/999]   Loss: 5.0615\n",
      "[VAL]  Epoch [49/999]   Loss: 4.9103\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [50/999]   Loss: 5.0137\n",
      "[VAL]  Epoch [50/999]   Loss: 4.9209\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [51/999]   Loss: 4.9881\n",
      "[VAL]  Epoch [51/999]   Loss: 4.9033\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [52/999]   Loss: 4.9485\n",
      "[VAL]  Epoch [52/999]   Loss: 4.8246\n",
      "Saving model, predictions and generated output for epoch 50 with NLL: 4.82456\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [53/999]   Loss: 4.9288\n",
      "[VAL]  Epoch [53/999]   Loss: 4.9145\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [54/999]   Loss: 4.9186\n",
      "[VAL]  Epoch [54/999]   Loss: 4.8423\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [55/999]   Loss: 4.8825\n",
      "[VAL]  Epoch [55/999]   Loss: 4.8579\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [56/999]   Loss: 4.8666\n",
      "[VAL]  Epoch [56/999]   Loss: 4.8557\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [57/999]   Loss: 4.8349\n",
      "[VAL]  Epoch [57/999]   Loss: 4.8611\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [58/999]   Loss: 4.8240\n",
      "[VAL]  Epoch [58/999]   Loss: 4.8307\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [59/999]   Loss: 4.7988\n",
      "[VAL]  Epoch [59/999]   Loss: 4.8059\n",
      "Saving model, predictions and generated output for epoch 57 with NLL: 4.8059273\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [60/999]   Loss: 4.7856\n",
      "[VAL]  Epoch [60/999]   Loss: 4.8020\n",
      "Saving model, predictions and generated output for epoch 58 with NLL: 4.801965\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [61/999]   Loss: 4.7788\n",
      "[VAL]  Epoch [61/999]   Loss: 4.8027\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [62/999]   Loss: 4.7700\n",
      "[VAL]  Epoch [62/999]   Loss: 4.7911\n",
      "Saving model, predictions and generated output for epoch 60 with NLL: 4.791072\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [63/999]   Loss: 4.7619\n",
      "[VAL]  Epoch [63/999]   Loss: 4.8083\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [64/999]   Loss: 4.7520\n",
      "[VAL]  Epoch [64/999]   Loss: 4.8096\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [65/999]   Loss: 4.7326\n",
      "[VAL]  Epoch [65/999]   Loss: 4.7840\n",
      "Saving model, predictions and generated output for epoch 63 with NLL: 4.784046\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [66/999]   Loss: 4.7222\n",
      "[VAL]  Epoch [66/999]   Loss: 4.7583\n",
      "Saving model, predictions and generated output for epoch 64 with NLL: 4.7582703\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [67/999]   Loss: 4.7171\n",
      "[VAL]  Epoch [67/999]   Loss: 4.7856\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [68/999]   Loss: 4.7068\n",
      "[VAL]  Epoch [68/999]   Loss: 4.7765\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [69/999]   Loss: 4.6902\n",
      "[VAL]  Epoch [69/999]   Loss: 4.7766\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [70/999]   Loss: 4.6832\n",
      "[VAL]  Epoch [70/999]   Loss: 4.7748\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [71/999]   Loss: 4.6693\n",
      "[VAL]  Epoch [71/999]   Loss: 4.7779\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [72/999]   Loss: 4.6627\n",
      "[VAL]  Epoch [72/999]   Loss: 4.7769\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [73/999]   Loss: 4.6496\n",
      "[VAL]  Epoch [73/999]   Loss: 4.7754\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [74/999]   Loss: 4.6453\n",
      "[VAL]  Epoch [74/999]   Loss: 4.7546\n",
      "Saving model, predictions and generated output for epoch 72 with NLL: 4.7545586\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [75/999]   Loss: 4.6407\n",
      "[VAL]  Epoch [75/999]   Loss: 4.7803\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [76/999]   Loss: 4.6359\n",
      "[VAL]  Epoch [76/999]   Loss: 4.7716\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [77/999]   Loss: 4.6254\n",
      "[VAL]  Epoch [77/999]   Loss: 4.7680\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [78/999]   Loss: 4.6221\n",
      "[VAL]  Epoch [78/999]   Loss: 4.7730\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [79/999]   Loss: 4.6130\n",
      "[VAL]  Epoch [79/999]   Loss: 4.7708\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [80/999]   Loss: 4.6101\n",
      "[VAL]  Epoch [80/999]   Loss: 4.7720\n",
      "batch_num:  0\n",
      "batch_num:  50\n",
      "batch_num:  100\n",
      "batch_num:  150\n",
      "batch_num:  200\n",
      "[TRAIN]  Epoch [81/999]   Loss: 4.6035\n",
      "[VAL]  Epoch [81/999]   Loss: 4.7615\n",
      "batch_num:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d424a62b4676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-72498cb1bd49>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_num: \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-72498cb1bd49>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#torch.nn.utils.clip_grad_norm_(list(self.model.parameters())+list(self.criterion.parameters()), 0.25)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | the first of the first @-@ known to the American\n",
      "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | @-@ = = = <eol> The first leg of the\n",
      "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | season in the United States . <eol> = = =\n",
      "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | . <eol> = = = = = <eol> = =\n",
      "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , and <unk> . <eol> = = = = =\n",
      "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the Atlantic Ocean , the highway was transferred to\n",
      "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = = <eol> = =\n",
      "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be a major centre . <eol> = = = <unk>\n",
      "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = = = <eol> = =\n",
      "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the company was transferred to the United States Navy\n",
      "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a <unk> of the <unk> of the <unk> , and\n",
      "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . The <unk> of the <unk> is\n",
      "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States , the first season of the season\n",
      "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | April , and the first year of the year ,\n",
      "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | 's <unk> to the <unk> of the <unk> , and\n",
      "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | of the <unk> . <eol> The first two years of\n",
      "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The first known of the first season , the series\n",
      "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 ) . <eol> = = = = =\n",
      "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the <unk> was the first time of the British Empire\n",
      "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the United States , and the first @-@ known\n",
      "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 ft ) . <eol> = = = <unk> =\n",
      "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | area of the church was not yet to be the\n",
      "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , the first time of the city\n",
      "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | . The storm was upgraded to the southeast of the\n",
      "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | . The <unk> is a large component of the fungus\n",
      "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were the first most @-@ watched game in the United\n",
      "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first known of the season was\n",
      "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" , and that \" the \" <unk> \" ,\n",
      "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = <unk> = = = <eol>\n",
      "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the game was \" <unk> \" . <eol> =\n",
      "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . <eol> = = = <unk> = = = <eol>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
